using Distributions
using Random
Random.seed!(0)

# Î¸â‚ = Beta(1, 1)
# Î¸â‚‚ = Beta(1, 1)


"""
Returns a real number value in the range `[0,1]` with
probability defined by a PDF of a Beta with parameters `a` and `b`.
"""
sampleBeta(a, b) = rand(Beta(a,b))


"""
Gives drug `i` to the next patient. Returns `true` if 
the grud was successful in curing the patient of `false`
if it was not. Throws an error if `i âˆ‰ {1,2}`.
"""
function giveDrug(i)
    # return rand([true, false, false])
    return [true, false][i]
end

# Attempt at general Thompson sampling from the TS Tutorial (algorithm 4).
function thompson_sampling(ğ’³, ğ’®, P, T, R; t_max=100) #, ğ›ƒ::Vector{Beta}, ğ›‰::Vector=ones(length()))
    for t in 1:t_max
        # sample model
        Î¸ = rand(P)

        # select and apply action
        xâ‚œ = argmax([sum(T(s, x, Î¸)*R(s) for s in ğ’®) for x in ğ’³])
        sâ‚œ = apply(xâ‚œ)

        # update distribution
        P = P(Î¸)*T(sâ‚œ, xâ‚œ) / sum(P(Î½)*T(Î½, xâ‚œ))
    end

    return P
end

X = [1, 1, 1, -1, -1, 1, 1, 1]
S = [1, 2, 3,  2,  1, 2, 3, 4]
P = Beta()
thompson_sampling()






### Huffman coding
function tree_print(N, v="")
    S = ""
    if N isa Char
        return v*"-"
    else
        S *= tree_print(N.left, v*"0")
        S *= tree_print(N.right, v*"1")
    end
    return S
end

# Not multiple dispatched.
function encode(x, N, v=0x0)
    if N isa Char
        return N == x ? v : 0x0
    else
        return encode(x, N.left, v<<1) | encode(x, N.right, v<<1+0x1)
    end
end

function encode(x, N, v="")
    if N isa Char
        return N == x ? v : ""
    else
        return encode(x, N.left, v*"") * encode(x, N.right, v*"1")
    end
end


encode(x, N::Char, v) = N == x ? v : 0b0
encode(x, N, v=0b0) = encode(x, N.left, v<<1) | encode(x, N.right, v<<1+0b1)


encode(s::String) = join(map(c->encode(c, N), collect(s)))
encode(c::Char, N, v="") = encode(c, N.left, v*'0') * encode(c, N.right, v*'1')
encode(c::Char, N::Char, v) = N == c ? v : ""

function decode(S::String, N, v="", next=N)
    for s in S
        next = (next isa Char) ? N : next
        next = (s == '0') ? next.left : next.right
        v *= (next isa Char) ? next : ""
    end
    return v
end


# Loss functions

âˆ‡trainloss(ğ°, ğ’Ÿtrain, Ï†, âˆ‡loss) = mean(âˆ‡loss(x, y, ğ°, Ï†) for (x,y) âˆˆ ğ’Ÿtrain)

#=
binary_classifier(x, ğ°, Ï†) = yÌ‚(x, ğ°, Ï†) â‰¥ 0 ? +1 : -1


loss_01(x, y, ğ°, Ï†)       = ğ•€(margin(x, y, ğ°, Ï†) â‰¤ 0)
loss_squared(x, y, ğ°, Ï†)  = residual(x, y, ğ°, Ï†)^2
loss_absdev(x, y, ğ°, Ï†)   = abs(residual(x, y, ğ°, Ï†))
loss_hinge(x, y, ğ°, Ï†)    = max(1 - margin(x, y, ğ°, Ï†), 0)
loss_logistic(x, y, ğ°, Ï†) = log(1 + exp(-margin(x, y, ğ°, Ï†)))
loss_cross_entropy(x, y, ğ°, Ï†; yÌ‚=prediction(x, ğ°, Ï†)) = -(y*log(yÌ‚) + (1-y)*log(1-yÌ‚))

âˆ‡loss_squared(x, y, ğ°, Ï†) = 2residual(x, y, ğ°, Ï†)*Ï†(x)
âˆ‡loss_hinge(x, y, ğ°, Ï†)   = margin(x, y, ğ°, Ï†) < 1 ? -Ï†(x)*y : 0

function gradient_descent(ğ’Ÿtrain, Ï†, âˆ‡trainloss; Î·=0.1, T=100)
    ğ° = init_weights(length(Ï†(ğ’Ÿtrain[1][1])))
    for t in 1:T
        ğ° = ğ° - Î·*âˆ‡trainloss(ğ°, ğ’Ÿtrain, Ï†)
    end
    return ğ°
end


function stochastic_gradient_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, T=100)
    ğ° = init_weights(length(Ï†(ğ’Ÿtrain[1][1])))
    for t in 1:T
        for (x,y) âˆˆ ğ’Ÿtrain
            ğ° = ğ° - Î·*âˆ‡loss(x, y, ğ°, Ï†)
        end
    end
    return ğ°
end
=#

# WRONG.
# margin(x, y, ğ°, Ï†) < 1 ? -Ï†(x)*y : Ï†(x)*y




# One-line two-layer neural network
neural_network(x, ğ•, ğ°, Ï†, g) = ğ° â‹… map(ğ¯â±¼ -> g(ğ¯â±¼ â‹… Ï†(x)), ğ•)


# From NN notebook

TikzPicture("""
\\tikzstyle{every pin edge}=[<-,shorten <=1pt]
\\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
\\tikzstyle{input neuron}=[neuron, fill=green!50];
\\tikzstyle{output neuron}=[neuron, fill=red!50];
\\tikzstyle{hidden neuron}=[neuron, fill=blue!50];
\\tikzstyle{annot} = [text width=4em, text centered]

% Draw the input layer nodes
\\foreach \\name / \\y in {1,...,3}
    \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {};

% Draw the hidden layer nodes
\\foreach \\name / \\y in {1,...,5}
    \\path[yshift=0.5cm]
        node[hidden neuron] (H-\\name) at (\\layersep,-\\y cm) {};

% Draw the output layer node
\\node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};

% Connect every node in the input layer with every node in the hidden layer.
\\foreach \\source in {1,...,3}
    \\foreach \\dest in {1,...,5}
        \\path (I-\\source) edge (H-\\dest);

% Connect every node in the hidden layer with the output layer
\\foreach \\source in {1,...,5}
    \\path (H-\\source) edge (O);

% Annotate the layers
\\node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
\\node[annot,left of=hl] {Input layer};
\\node[annot,right of=hl] {Output layer};
""",
options="shorten >=1pt,->,draw=black!50, node distance=\\layersep",
preamble="\\def\\layersep{2.5cm}")


using TikzGraphs, LightGraphs, MetaGraphs

begin
    g = DiGraph(6)
    add_edge!(g, 1, 4)
    add_edge!(g, 1, 5)
    add_edge!(g, 2, 4)
    add_edge!(g, 2, 5)
    add_edge!(g, 3, 4)
    add_edge!(g, 3, 5)
    add_edge!(g, 4, 6)
    add_edge!(g, 5, 6)
end

graph = TikzGraphs.plot(g, Layouts.Layered(), node_style="circle, draw=black, fill=white, minimum size=16pt", options="grow'=right, level distance=15mm, sibling distance=2mm, semithick, >=stealth'");

graph.width="12cm"; graph

gnn = TikzGraphs.plot(g, Layouts.Layered(), ["", "", "", L"\sigma", L"\sigma", ""], node_style="circle, draw=black, fill=white, minimum size=16pt",
node_styles=Dict(1=>"label=left:{\$\\phi(x)_1\$}",2=>"label=left:{\$\\phi(x)_2\$}", 3=>"label=left:{\$\\phi(x)_3\$}", 4=>"fill=lightgray!70", 5=>"fill=lightgray!70"),
options="grow'=right, level distance=15mm, sibling distance=2mm, semithick, >=stealth'");

gnn.width="12cm"; gnn



        dW2 = 1/m * dZ2*A1'
        db2 = 1/m * sum(dZ2)
        dZ1 = W2'*dZ2 .* (1 .- A1.^2)
        dW1 = 1/m * dZ1*X'
        db1 = 1/m * sum(dZ1)

    nn.grads["dW1"] = dW1
    nn.grads["db1"] = db1
    nn.grads["dW2"] = dW2
    nn.grads["db2"] = db2



function backpropagation!(nn, X, Y)
    m = size(X)[2]

    Aj = nn.forward["A$(length(nn.activation_functions)-1)"]
    Wj = nn.Î¸["W$(length(nn.activation_functions)-1)"]
    local dZj
    for i in length(nn.activation_functions):-1:1
        Wi = nn.Î¸["W$i"]
        Ai = nn.forward["A$i"]
        
        if i == length(nn.activation_functions)
            dZi = Ai .- Y # assumes binary classification with sigmoid
        else
            dZi = Wj' * dZj .* nn.activation_functionsâ€².(Aj)
        end
        dWi = 1/m * dZi*Aj'
        dbi = 1/m * sum(dZi)
        Aj = i == 2 ? X : Ai
        dZj = dZi
    
        nn.grads["dW$i"] = dWi
        nn.grads["db$i"] = dbi
    end
end


# manual backpropagation
    dZ2 = A2 .- Y # assumes binary classification with sigmoid
    dW2 = 1/m * dZ2*A1'
    db2 = 1/m * sum(dZ2)
    dZ1 = W2'*dZ2 .* (1 .- A1.^2)
    dW1 = 1/m * dZ1*X'
    db1 = 1/m * sum(dZ1)
    
    nn.grads["dW1"] = dW1
    nn.grads["db1"] = db1
    nn.grads["dW2"] = dW2
    nn.grads["db2"] = db2

function backpropagation!(nn, X, Y)
    m = size(X)[2]

    W1 = nn.Î¸["W1"]
    W2 = nn.Î¸["W2"]
    
    A1 = nn.forward["A1"]
    A2 = nn.forward["A2"]
    
    # for each activation, get the gradient
    # dW2 = first(gradient(a->loss(a, Y), W2))

    # TODO: Generalize.
    dZ2 = A2 .- Y # assumes binary classification with sigmoid
    dW2 = 1/m * dZ2*A1'
    db2 = 1/m * sum(dZ2)
    dZ1 = W2'*dZ2 .* (1 .- A1.^2)
    dW1 = 1/m * dZ1*X'
    db1 = 1/m * sum(dZ1)
    
    nn.grads["dW1"] = dW1
    nn.grads["db1"] = db1
    nn.grads["dW2"] = dW2
    nn.grads["db2"] = db2
end


function backpropagation!(nn, AL, Y)
    L = length(nn.hidden_layer_sizes)
    
    dA = âˆ‡loss.(AL, Y)
    
    for l in reverse(0:L-1)
        A, Z = nn.forward["A$(l+1)"], nn.forward["Z$(l+1)"]
        W, b = nn.Î¸["W$(l+1)"], nn.Î¸["b$(l+1)"]
        m = size(A)[2]
        gâ€² = nn.activation_functionsâ€²[l+1]
        dZ = dA âŠ™ gâ€².(Z)
        dA = W' * dZ
        dW = 1/m * dZ*A'
        db = 1/m * sum(dZ)
        nn.grads["dA$l"] = dA
        nn.grads["dW$l"] = dW
        nn.grads["db$l"] = db
    end
end

function backpropagation!(nn, AL, Y)
    L = length(nn.hidden_layer_sizes)
    
    A = nn.forward["A$L"]
    Z = nn.forward["Z$L"]
    m = size(A)[2]
    dA = âˆ‡loss.(A, Y) # TODO: AL
    dZ = nn.activation_functionsâ€²[L].(dA)
    dW = 1/m * dZ*A'
    db = 1/m * sum(dZ)
    
    for l in reverse(1:L-1)
        m = size(A)[2]
        dW = 1/m * dZ*A'
        db = 1/m * sum(dZ)
        gâ€² = nn.activation_functionsâ€²[l]
        dZ = dA âŠ™ gâ€².(Z)
        nn.grads["dA$l"] = dA
        nn.grads["dW$(l+1)"] = dW
        nn.grads["db$(l+1)"] = db
        A, Z = nn.forward["A$l"], nn.forward["Z$l"]
        W = nn.Î¸["W$(l+1)"]
        dA = W' * dZ
    end
end


## WORKS----with Coursera-style breakup of functions
function backpropagation!(nn, AL, Y)
    L = length(nn.hidden_layer_sizes)
    
    dAL = âˆ‡loss.(AL, Y)
    A_prev, W, b = nn.forward["A$L"], nn.Î¸["W$L"], nn.Î¸["b$L"]
    Z = nn.forward["Z$L"]
    nn.grads["dA$L"], nn.grads["dW$L"], nn.grads["db$L"] = 
        linear_activation_backward(dAL, Z, A_prev, W, b, nn.activation_functionsâ€²[L])
    
    for l in reverse(0:L-2)     
        Z = nn.forward["Z$(l+1)"]
        A_prev, W, b = nn.forward["A$(l+1)"], nn.Î¸["W$(l+1)"], nn.Î¸["b$(l+1)"]
        dA_prev_tmp, dW_tmp, db_tmp = 
            linear_activation_backward(nn.grads["dA$(l+2)"], Z, A_prev, W, b,
                                       nn.activation_functionsâ€²[l+1])
        nn.grads["dA$(l+1)"] = dA_prev_tmp
        nn.grads["dW$(l+1)"] = dW_tmp
        nn.grads["db$(l+1)"] = db_tmp
        # m = size(A)[2]
        # dW = 1/m * dZ*A'
        # db = 1/m * sum(dZ)
        # gâ€² = nn.activation_functionsâ€²[l]
        # dZ = dA âŠ™ gâ€².(Z)
        # nn.grads["dA$l"] = dA
        # nn.grads["dW$(l+1)"] = dW
        # nn.grads["db$(l+1)"] = db
        # A, Z = nn.forward["A$l"], nn.forward["Z$l"]
        # W = nn.Î¸["W$(l+1)"]
        # dA = W' * dZ
    end
end

function linear_backward(dZ, A_prev, W, b)
    m = size(A_prev)[2]
    dW = 1/m * dZ*A_prev'
    db = 1/m * sum(dZ; dims=2)
    dA_prev = W' * dZ
    
    return dA_prev, dW, db
end

function linear_activation_backward(dA, Z, A_prev, W, b, gâ€²)
    dZ = dA âŠ™ gâ€².(Z)
    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)
    return dA_prev, dW, db
end


function backpropagation!(nn, ğ€L, ğ˜)
    L = length(nn.hidden_layer_sizes)

    nn.dğ€[L+1] = âˆ‡loss.(ğ€L, ğ˜)

    for l in reverse(1:L)
        (ğ–, ğ›) = nn.ğ–[l], nn.ğ›[l]
        (ğ€, ğ™, dğ€) = nn.ğ€[l], nn.ğ™[l], nn.dğ€[l+1]
        gâ€² = nn.ğ â€²[l]

        m = size(ğ€)[2]
        dğ™ = dğ€ âŠ™ gâ€².(ğ™)
        dğ– = 1/m * dğ™*ğ€'
        dğ› = 1/m * sum(dğ™; dims=2)
        dğ€ = ğ–'dğ™

        nn.dğ€[l], nn.dğ–[l], nn.dğ›[l] = dğ€, dğ–, dğ›
    end
end

function backpropagation!(nn, ğ€, ğ˜)
    L = length(nn.hidden_layer_sizes)

    nn.dğ€[L+1] = âˆ‡loss.(ğ€, ğ˜)

    for l in reverse(1:L)
        m = size(ğ€)[2]
        dğ™ = nn.dğ€[l+1] âŠ™ nn.ğ â€²[l].(nn.ğ™[l])
        dğ– = 1/m * dğ™*nn.ğ€[l]'
        dğ› = 1/m * sum(dğ™; dims=2)
        dğ€ = nn.ğ–[l]'dğ™

        nn.dğ€[l], nn.dğ–[l], nn.dğ›[l] = dğ€, dğ–, dğ›
    end
end


function forwardpass!(nn, ğ—)
    (ğ–, ğ›, ğ™, ğ€, ğ ) = (nn.ğ–, nn.ğ›, nn.ğ™, nn.ğ€, nn.ğ )
    ğ€[1] = ğ—
    for (l,g) in enumerate(ğ )
        ğ™[l] = ğ–[l]*ğ€[l] .+ ğ›[l] # linear forward
        ğ€[l+1] = g.(ğ™[l]) # apply activation
    end 
    return ğ€[end-1]
end

function cost(nn::NeuralNetwork, ğ˜)
    N = length(nn.ğ )
    ğ€â‚™ = nn.ğ€[N]
    return cost(ğ€â‚™, ğ˜)
end


md"""
## Backpropagation

$$\begin{align}
d\mathbf{Z}^{[L]} &= \mathbf{A}^{[L]} - \mathbf{Y}\\
d\mathbf{W}^{[L]} &= \frac{1}{m} d\mathbf{Z}^{[L]}\mathbf{A}^{[L-1]\top}\\
d\mathbf{b}^{[L]} &= \frac{1}{m} \sum d\mathbf{Z}^{[L]}\\
d\mathbf{Z}^{[L-1]} &= \mathbf{W}^{[L]\top} d\mathbf{Z}^{[L]} \odot g^{\prime[L-1]}(\mathbf{Z}^{[L-1]})\\
&\vdots
\end{align}$$
"""