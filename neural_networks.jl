### A Pluto.jl notebook ###
# v0.11.14

using Markdown
using InteractiveUtils

# â•”â•â•¡ 0b7610be-fea4-11ea-0a69-eff0a019b6e6
using LinearAlgebra, TikzPictures

# â•”â•â•¡ 32b06dc0-fef4-11ea-25f2-cdaeafd52ab4
using TikzNeuralNetworks

# â•”â•â•¡ 3945e080-ffac-11ea-23a8-b968fe84d903
using Statistics # LinearAlgebra

# â•”â•â•¡ 3641cda0-ffbf-11ea-27a3-5fe417918cc0
using AddPackage

# â•”â•â•¡ 991f6770-ffbf-11ea-048c-1113d8640637
@add using PyPlot, ScikitLearn, Random, Parameters

# â•”â•â•¡ 0414b960-ffd7-11ea-313c-3fbce53d7a70
using Zygote

# â•”â•â•¡ dbd0ccc0-fea3-11ea-359e-876ee15179be
md"# Neural networks"

# â•”â•â•¡ 047d3eb2-fea4-11ea-3ff7-0b15c54073e2
md"## Activation functions"

# â•”â•â•¡ 08c79290-fea4-11ea-0a77-e5f5e6ab401d
ReLU(z) = max(z, 0)

# â•”â•â•¡ c8f6b2c0-ffd1-11ea-3972-25233fa64f94
ReLUâ€²(z) = z < 0 ? 0 : 1

# â•”â•â•¡ 1384e6b0-fea4-11ea-2e03-cdad79f986d5
Ïƒ(z) = 1/(1 + exp(-z))

# â•”â•â•¡ ae23a200-ffd1-11ea-3cfe-bb895cf90087
Ïƒâ€²(z) = Ïƒ(z)*(1 - Ïƒ(z))

# â•”â•â•¡ be738170-ffd1-11ea-23ec-45adb917e9c6
tanhâ€²(z) = 1 - tanh(z)^2

# â•”â•â•¡ 233ec7b0-fea4-11ea-3a47-a3125ec028be
md"## Two-layer neural network"

# â•”â•â•¡ a13c0ba0-fea4-11ea-0fa7-cde0a3840c99
TikzPicture(L"""
\node (phi2) [nnnode, label=left:{$\phi(x)_2$}] {};
\node (phi1) [nnnode, above=2mm of phi2, label=left:{$\phi(x)_1$}] {};
\node (phi3) [nnnode, below=2mm of phi2, label=left:{$\phi(x)_3$}] {};

\node (hidden1) [nnnode, above right=0mm and 15mm of phi2, label=above:{$h_1$}] {$\darkblue{\sigma}$};
\node (hidden2) [nnnode, below right=0mm and 15mm of phi2, label=below:{$h_2$}] {$\darkblue{\sigma}$};

\node (score) [nnnode, right=30mm of phi2, label=above:{score}] {};

\draw[->] (phi1) -- (hidden1) node [midway, yshift=7pt] {$\darkred{\mathbf{V}}$};
\draw[->] (hidden1) -- (score) node [midway, yshift=7pt] {$\darkred{\mathbf{w}}$};
\draw[->] (phi1) -- (hidden2);
\draw[->] (phi2) -- (hidden1);
\draw[->] (phi2) -- (hidden2);
\draw[->] (phi3) -- (hidden1);
\draw[->] (phi3) -- (hidden2);
\draw[->] (hidden1) -- (score);
\draw[->] (hidden2) -- (score);	
""",
preamble="""
\\definecolor{stanfordred}{RGB}{140,21,21}
\\definecolor{darkblue}{RGB}{21,21,140}
\\newcommand{\\darkblue}[1]{{\\color{darkblue} #1}}
\\newcommand{\\darkred}[1]{{\\color{stanfordred} #1}}
\\usetikzlibrary{positioning}
\\usetikzlibrary{arrows}
\\tikzset{nnnode/.style = {circle, draw=black, fill=white, minimum size=16pt,},}
\\tikzset{every picture/.style={semithick, >=stealth'}}
""",
width="12cm"
)

# â•”â•â•¡ 11ab44b0-fea4-11ea-042f-a3547f3e2a8b
function neural_network(x, ğ•, ğ°, Ï†, g=ReLU)
    ğ¡ = map(ğ¯â±¼ -> g(ğ¯â±¼ â‹… Ï†(x)), ğ•)
    ğ° â‹… ğ¡
end

# â•”â•â•¡ 12cfa070-fea4-11ea-0963-9548b2c07c44
md"## Multi-layer neural network
Also called a *multi-layer perceptron*."

# â•”â•â•¡ 3c132240-fea4-11ea-0b96-8fac4ceb5841
function multi_layer_neural_network(x, ğ–, Ï†, ğ )
    ğ¡áµ¢ = Ï†(x)
    for (i,g) in enumerate(ğ )
        ğ¡áµ¢ = map(ğ°â±¼ -> g(ğ°â±¼ â‹… ğ¡áµ¢), ğ–[i])
    end
    ğ¡áµ¢ â‹… last(ğ–)
end

# â•”â•â•¡ a43062c0-0020-11eb-245e-cf6b2036b878
function plot_nn()
	nn2 = TikzNeuralNetwork(input_size=2,
		input_label=i->"\$x_{$i}\$",
		input_arrows=false,
		hidden_layer_sizes=[4, 1],
		activation_functions=[L"g", L"\sigma"],
		hidden_layer_labels=[L"\tanh", "logistic"],
		output_label=i->L"\hat{y}",
		output_arrows=false,
		output_size=1)
	nn2.tikz.width="12cm"
	return nn2
end

# â•”â•â•¡ b17c5100-0020-11eb-2928-81d80731960e
plot_nn()

# â•”â•â•¡ c43d5c20-ff9f-11ea-15ce-3bef26583d2a
md"""
$$\begin{gather}
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \mathbf{a}^{[0]} = (3\times1) \text{ vector}
\end{gather}$$
"""

# â•”â•â•¡ 492db7ae-ff9e-11ea-38a0-09cead1c5ae6
md"""
For weights $\mathbf{W}$:

$$\begin{gather}
\mathbf{W}^{[1]} = \begin{bmatrix}
& â€” & \mathbf{w}_1^{[1]\top} & â€” & \\
& â€” & \mathbf{w}_2^{[1]\top} & â€” & \\
& â€” & \mathbf{w}_3^{[1]\top} & â€” & \\
& â€” & \mathbf{w}_4^{[1]\top} & â€” & \\
\end{bmatrix} = (|\mathbf{a}^{[1]}| \times |\mathbf{x}|) = (4\times3) \text{ matrix}\\
\end{gather}$$
"""

# â•”â•â•¡ e7ffb7c0-ff9f-11ea-3c0e-cbb878af2c6e
md"""
$$\begin{gather}
\mathbf{b}^{[1]} = \begin{bmatrix} b_1^{[1]} \\ b_2^{[1]} \\ b_3^{[1]} \\ b_4^{[1]} \end{bmatrix} = (|\mathbf{a}^{[1]}| \times 1) = (4\times1) \text{ vector}
\end{gather}$$
"""

# â•”â•â•¡ 180dc51e-ff9f-11ea-3ad0-a51012f662b0
md"""
$$\begin{gather}
\mathbf{z}^{[1]} = 
\mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]} = 
\begin{bmatrix}
\mathbf{w}_1^{[1]\top} \mathbf{x} + b_1^{[1]}\\
\mathbf{w}_2^{[1]\top} \mathbf{x} + b_2^{[1]}\\
\mathbf{w}_3^{[1]\top} \mathbf{x} + b_3^{[1]}\\
\mathbf{w}_4^{[1]\top} \mathbf{x} + b_4^{[1]}\\
\end{bmatrix} = 
\begin{bmatrix}
z_1^{[1]} \\ z_2^{[1]} \\ z_3^{[1]} \\ z_4^{[1]} \\
\end{bmatrix} = (4\times1) \text{ vector}
\end{gather}$$
"""

# â•”â•â•¡ 07054ea0-ffa0-11ea-1fd1-6f70c3600784
md"""
$$\begin{gather}
\mathbf{a}^{[1]} = \begin{bmatrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \end{bmatrix} = \sigma(\mathbf{z}^{[1]}) = (4\times1) \text{ vector}
\end{gather}$$
"""

# â•”â•â•¡ d75e9920-ffa1-11ea-1bb0-6d0424b2b5b5
md"""
Then for the next last hidden layer to output layer.

$$\begin{gather}
\mathbf{W}^{[2]} = \mathbf{w}^\top = \begin{bmatrix} w_1^{[2]} &  w_1^{[2]} & w_1^{[2]} & w_1^{[2]} \end{bmatrix} = (1\times4) \text{ matrix}\\
\mathbf{z}^{[2]} = \mathbf{W}^{[2]} \mathbf{a}^{[1]} + b^{[2]} = (1\times1) \text{ vector}\\
\mathbf{a}^{[2]} = \sigma(\mathbf{z}^{[2]}) = \hat{y}
\end{gather}$$
"""

# â•”â•â•¡ b70c8f9e-ffa2-11ea-1f51-cd5660e3eb37
md"""
## Vectorization

$$\begin{align}
\mathbf{z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]}\\
\mathbf{a}^{[1]} &= \sigma(\mathbf{z}^{[1]})\\
\mathbf{z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{a}^{[1]} + \mathbf{b}^{[2]}\\
\mathbf{a}^{[2]} &= \sigma(\mathbf{z}^{[2]})
\end{align}$$

Vectorizing across multiple examples $i$.

$$\begin{align}
\texttt{for i }&\texttt{in 1 to m:}\\
\mathbf{z}^{[1](i)} &= \mathbf{W}^{[1]} \mathbf{x}^{(i)} + \mathbf{b}^{[1]}\\
\mathbf{a}^{[1](i)} &= \sigma(\mathbf{z}^{[1](i)})\\
\mathbf{z}^{[2](i)} &= \mathbf{W}^{[2]} \mathbf{a}^{[1](i)} + \mathbf{b}^{[2]}\\
\mathbf{a}^{[2](i)} &= \sigma(\mathbf{z}^{[2](i)})
\end{align}$$
"""

# â•”â•â•¡ 0e09c0d0-ff9d-11ea-0e5e-37f227129043
md"""
For $m$ training examples, the input data $\mathbf{X}$ is represented as a matrix.

$$\begin{gather}
\mathbf{X} = \begin{bmatrix}
\mid & \mid & & \mid\\
\mathbf{x}^{(1)} & \mathbf{x}^{(2)} & \cdots & \mathbf{x}^{(m)}\\
\mid & \mid & & \mid\\
\end{bmatrix} = (n_x \times m) = (3 \times m) \text{ matrix}\\
\end{gather}$$
"""

# â•”â•â•¡ e3dd9740-ff9d-11ea-0d04-ad6f8be98423
md"""
$$\begin{gather}
\mathbf{Z}^{[1]} = \begin{bmatrix}
\mid & \mid & & \mid\\
\mathbf{z}^{[1](1)} & \mathbf{z}^{[1](2)} & \cdots & \mathbf{z}^{[1](m)}\\
\mid & \mid & & \mid\\
\end{bmatrix} = (|\mathbf{z}^{[1]}| \times m) = (4\times m) \text{ matrix}\\
\end{gather}$$
"""

# â•”â•â•¡ 385750e0-ffa3-11ea-03d1-a7cc682d4ab2
md"""
$$\begin{gather}
\mathbf{A}^{[1]} = \begin{bmatrix}
\mid & \mid & & \mid\\
\mathbf{a}^{[1](1)} & \mathbf{a}^{[1](2)} & \cdots & \mathbf{a}^{[1](m)}\\
\mid & \mid & & \mid\\
\end{bmatrix} = (|\mathbf{a}^{[1]}| \times m) = (4 \times m) \text{ matrix}\\
\end{gather}$$
"""

# â•”â•â•¡ e2891c10-ffa3-11ea-05a6-f93d0bd8daf8
md"""
Vectorized over all training examples.

$$\begin{align}
\mathbf{Z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{X} + \mathbf{b}^{[1]}\\
\mathbf{A}^{[1]} &= \sigma(\mathbf{Z}^{[1]})\\
\mathbf{Z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]}\\
\mathbf{A}^{[2]} &= \sigma(\mathbf{Z}^{[2]})
\end{align}$$

Simplified to:

$$\begin{align}
\mathbf{Z}^{[\ell]} &= \mathbf{W}^{[\ell]} \mathbf{A}^{[\ell-1]} + \mathbf{b}^{[\ell]}\\
\mathbf{A}^{[\ell]} &= \sigma(\mathbf{Z}^{[\ell]})
\end{align}$$

Simplified further, with a generalized activation function $g$:

$$\begin{gather}
\mathbf{A}^{[\ell]} = g(\mathbf{W}^{[\ell]} \mathbf{A}^{[\ell-1]} + \mathbf{b}^{[\ell]})
\end{gather}$$
"""

# â•”â•â•¡ 66219680-ffa7-11ea-3528-9184d31f42de
md"""
## Activation functions

Activation functions $a = g(z)$:

For output layer, $\sigma$ works to map between $0-1$:

$$\begin{gather}
\sigma(z) = \frac{1}{1 + e^{-z}}\\
\end{gather}$$

Mean of data closer to zero with $\tanh$, usually works better than $\sigma$ as a hidden layer:

$$\begin{gather}
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\\
\end{gather}$$

The rectified linear unit:

$$\text{ReLU}(z) = \max(0, z)$$

The leaky $\text{ReLU}$:

$$\text{Leaky ReLU}(z) = \max(0.01 z, z)$$

For different activation functions $g^{[1]}$ for first hidden layer, $g^{[2]}$ for second hidden layer, etc.

**Rules to choose activation functions:**
- If output is $\{0, 1\}$, i.e. binary classification, sigmoid is a natural choice for the output layer $\sigma$. For all other layers, $\text{ReLU}$ is a good choice.

**Derivatives**

$$\begin{align}
\sigma^\prime(z) &= \sigma(z)(1 - \sigma(z))\\
\tanh^\prime(z) &= 1 - \tanh(z)^2\\
\text{ReLU}^\prime &= \begin{cases}
0 & \text{if } z < 0\\
1 & \text{if } z > 0\\
\text{undef} & \text{if } z = 0
\end{cases}\\
\text{Leaky ReLU}^\prime &= \begin{cases}
0.01 & \text{if } z < 0\\
1 & \text{if } z > 0\\
\text{undef} & \text{if } z = 0
\end{cases}
\end{align}$$
"""

# â•”â•â•¡ 30cb2962-ffac-11ea-114f-73601457cef0
md"""
## Gradient descent
"""

# â•”â•â•¡ c708c540-ffac-11ea-0632-35fdb0b7a4ef
md"""
$$\begin{gather}
n_x = n^{[0]}, n^{[1]}, n^{[2]} = 1 \tag{in this example}\\
\mathbf{W}^{[1]} = (n^{[1]} \times n^{[0]})\\
\mathbf{b}^{[1]} = (n^{[1]} \times 1)\\
\mathbf{W}^{[1]} = (n^{[2]} \times n^{[1]})\\
\mathbf{b}^{[2]} = (n^{[2]} \times 1)\\
\end{gather}$$

$$J(\theta) = J\left(\mathbf{W}^{[1]}, \mathbf{b}^{[1]}, \mathbf{W}^{[2]}, \mathbf{b}^{[2]}\right) = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(\hat{y}, y)$$

$$\partial \mathbf{W}^{[1]} = \frac{\partial J}{\partial \mathbf{W}^{[1]}},\quad
\partial \mathbf{b}^{[1]} = \frac{\partial J}{\partial \mathbf{b}^{[1]}}, \quad \ldots$$

Then the gradient update would be:

$$\begin{gather}
\mathbf{W}^{[1]} = \mathbf{W}^{[1]} - \alpha (\partial \mathbf{W}^{[1]})\\
\mathbf{b}^{[1]} = \mathbf{b}^{[1]} - \alpha (\partial \mathbf{b}^{[1]})\\
\ldots
\end{gather}$$
"""

# â•”â•â•¡ 596a9110-ffae-11ea-2c89-7518df5b04c6
md"""
### Backpropogation

[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)

$$\mathbf{Y} = \begin{bmatrix} y^{(1)} & y^{(2)} & \cdots & y^{(m)} \end{bmatrix}$$

$$\begin{gather}
\partial \mathbf{Z}^{[2]} = \mathbf{A}^{[2]} - \mathbf{Y}\\
\partial \mathbf{W}^{[2]} = \frac{1}{m} \partial \mathbf{Z}^{[2]} \mathbf{A}^{[1]\top}\\
\partial \mathbf{b}^{[2]} = \frac{1}{m} \sum \partial \mathbf{Z}^{[2]}\\
\partial \mathbf{Z}^{[1]} = \mathbf{W}^{[2]\top} \partial \mathbf{Z}^{[2]} \odot g^{[1]\prime}(\mathbf{Z}^{[1]}) \tag{derivative $g^\prime$}\\
(n^{[1]} \times m) \odot (n^{[1]} \times m) \tag{element-wise product}\\
\partial \mathbf{W}^{[1]} = \frac{1}{m} \partial \mathbf{Z}^{[1]}\mathbf{X}^\top\\
\partial \mathbf{b}^{[1]} = \frac{1}{m} \sum \partial \mathbf{Z}^{[1]} 
\end{gather}$$

`numpy`: `np.sim(dZ[1], axis=1, keepdims=True)`
"""


# â•”â•â•¡ 35d0b6f0-ffac-11ea-27b1-47b519c13c21
function gradient_descent(ğ’Ÿtrain, Ï†, âˆ‡loss; Î·=0.1, T=100)
    ğ° = 0.01randn(length(Ï†(ğ’Ÿtrain[1][1])))
    for t in 1:T
        ğ° = ğ° .- Î·*mean(âˆ‡loss(x, y, ğ°, Ï†) for (x,y) âˆˆ ğ’Ÿtrain)
    end
    return ğ°
end

# â•”â•â•¡ a2bcbdb0-ffa0-11ea-0a41-457fa1b70946
md"""
## Logistic regression
Simple neural network.

$$\begin{gather}
z = \mathbf{w}^\top \mathbf{x} + b\\
a = \sigma(z) = \hat{y}
\end{gather}$$
"""

# â•”â•â•¡ 68ca10f0-ff49-11ea-3147-09eb71687f20
begin
	logreg = TikzNeuralNetwork(input_size=3,
		input_arrows=false,
		input_label=i->"\$\\mathbf{x}_{$i}\$",
		activation_functions=["\$\\sigma(\\theta^\\top x)\$"],
		output_label=i->L"\hat{y}",
		output_arrows=false)
	logreg.tikz.width="10cm"
	logreg
end

# â•”â•â•¡ 24692920-ffb0-11ea-3ab2-47b00719deed
begin
	logreg_spanned = TikzNeuralNetwork(input_size=3,
		input_arrows=false,
		input_label=i->Dict(1=>L"x", 2=>L"w", 3=>L"b")[i],
		activation_functions=[L"z=w^\top x + b", L"a=\sigma(z)"],
		hidden_layer_sizes=[1,1],
		output_label=i->L"\mathcal{L}(a,y)",
		output_arrows=false)
	logreg_spanned.tikz.width="10cm"
	logreg_spanned.tikz.data = replace(logreg_spanned.tikz.data, "fill=lightgray!70"=>"fill=lightgray!70, rectangle")
	logreg_spanned
end

# â•”â•â•¡ 5a430f62-ffb1-11ea-2123-cd71772704ee
md"""
## Coursera data classification (port)
"""

# â•”â•â•¡ 7438c7c0-001a-11eb-1b09-b593715f3e82
PyPlot.svg(true);

# â•”â•â•¡ 55eb5030-ffc0-11ea-0688-c1ecd12f1939
@sk_import datasets: make_moons

# â•”â•â•¡ 98799310-ffd6-11ea-0714-17b135c4c704
@sk_import datasets: make_circles

# â•”â•â•¡ efce9b80-ffc0-11ea-2cb3-4f8209f29cdc
begin
	X, Y = make_moons(n_samples=200, noise=0.2)
 	# X, Y = make_circles(noise=0.2, factor=0.5, random_state=1)
	X, Y = X', reshape(Y, 1, size(Y)[1])
end

# â•”â•â•¡ 10fb310e-ffc1-11ea-0d21-539799c2aebf
size(X)

# â•”â•â•¡ 3d66f4f0-ffc1-11ea-18ae-a5063f76ded5
size(Y)

# â•”â•â•¡ 4016f9c0-ffc1-11ea-0638-8fc7e9f2c52b
clf(); plt.scatter(X[1,:], X[2,:], c=Y, s=40, cmap=plt.cm.Spectral); gcf()

# â•”â•â•¡ 3c930220-ffcc-11ea-0694-e73b06bf7623
@with_kw mutable struct NeuralNetwork
	input_size::Int = 2
	hidden_layer_sizes::Vector{Int} = [5, 1]
	activation_functions::Vector{Function} = [tanh, Ïƒ]
	activation_functionsâ€²::Vector{Function} = [z->(1 - tanh(z)^2), z->Ïƒ(z)*(1-Ïƒ(z))]
	output_size::Int = 1
	Î¸::Dict = Dict()
	forward::Dict = Dict()
	grads::Dict = Dict()
end

# â•”â•â•¡ 6c1a4160-ffcd-11ea-249a-79972ec4eb85
nn = NeuralNetwork()

# â•”â•â•¡ d8083910-ffc1-11ea-31f8-07d7fd4a0b9a
function initialize!(nn, X, Y)
	nn.input_size = size(X)[1]
	nn.output_size = size(Y)[1]
	nâ‚“ = nn.input_size
	for (i,nâ‚•) in enumerate(vcat(nn.hidden_layer_sizes, nn.output_size))
		Wi = 0.01randn(nâ‚•, nâ‚“)
		bi = zeros(nâ‚•, 1)
		nâ‚“ = nâ‚•
		nn.Î¸["W$i"] = Wi
		nn.Î¸["b$i"] = bi
	end
end

# â•”â•â•¡ 1aaf8430-ffc2-11ea-0520-451f13594ddc
function forwardpass!(nn, X)
	for (i,g) in enumerate(nn.activation_functions)
		Wi = nn.Î¸["W$i"]
		bi = nn.Î¸["b$i"]
		Zi = Wi*X .+ bi
		Ai = g.(Zi)
		X = Ai # for next iteration
		nn.forward["Z$i"] = Zi
		nn.forward["A$i"] = Ai
	end
end

# â•”â•â•¡ b6d126c0-ffc2-11ea-2400-9bd611d2d8b7
function cost(nn, Y)
	N = length(nn.activation_functions)
	Aâ‚™ = nn.forward["A$N"]
	return -mean(Y .* log.(Aâ‚™) .+ (1 .- Y) .* log.(1 .- Aâ‚™))
end

# â•”â•â•¡ 92237430-ffd7-11ea-3356-d9cd94e2a587
function cost2(Aâ‚™, Y)
	return -mean(Y .* log.(Aâ‚™) .+ (1 .- Y) .* log.(1 .- Aâ‚™))
end

# â•”â•â•¡ a19c1100-ffd8-11ea-37d1-b3f2e0b6dc0f
linear(Î¸, x) = Î¸.W*x .+ Î¸.b

# â•”â•â•¡ ea7a6f60-ffd9-11ea-395c-f769cabcc23f
loss(a, y) = -(y .* log.(a) .+ (1 .- y) .* log(1 .- a))

# â•”â•â•¡ ba7f4af0-0020-11eb-2112-e13c9ebaec6c
plot_nn()

# â•”â•â•¡ 835e1c70-ffc3-11ea-31a5-214731bbc366
function backpropagation!(nn, X, Y)
	m = size(X)[2]

	W1 = nn.Î¸["W1"]
	W2 = nn.Î¸["W2"]
	
	ğ€1 = nn.forward["A1"]
	ğ€2 = nn.forward["A2"]
	
	# for each activation, get the gradient
# 	Î¸1 = (W=nn.Î¸["W1"], b=nn.Î¸["b1"])
# 	Î¸2 = (W=nn.Î¸["W2"], b=nn.Î¸["b2"])
# 	d2 = first(gradient(a->mean(loss.(Ïƒ.(linear(a, ğ€1)), Y)), Î¸2))
# 	d1 = first(gradient(a->mean(tanh.(linear(a, X))), Î¸1))
# 	dW1 = Î¸2.W'*d2.W * tanh'.(nn.forward["Z1"])
# 	db1 = 1/m * sum(dZ1)

	# TODO: Generalize.
	dZ2 = ğ€2 .- Y # assumes binary classification with sigmoid
	dW2 = 1/m * dZ2*ğ€1'
	db2 = 1/m * sum(dZ2)
	dZ1 = W2'*dZ2 .* (1 .- ğ€1.^2)
	dW1 = 1/m * dZ1*X'
	db1 = 1/m * sum(dZ1)
	
	nn.grads["dW1"] = dW1
	nn.grads["db1"] = db1
	nn.grads["dW2"] = dW2
	nn.grads["db2"] = db2

# 	nn.grads["dW1"] = d1.W
# 	nn.grads["db1"] = d1.b

# 	nn.grads["dW1"] = dW1
# 	nn.grads["db1"] = db1
# 	nn.grads["dW2"] = d2.W
# 	nn.grads["db2"] = d2.b
end

# â•”â•â•¡ 24fd6bb0-0025-11eb-0489-5f1193474b18
md"""
$$\begin{align}
\theta &= \langle \mathbf{W}^{[j]}, \mathbf{b}^{[j]} \rangle\\
\mathbf{Z}^{[j]} &= \mathbf{W}^{[j]} \mathbf{A}^{[j-1]} + \mathbf{b}^{[j]}\\
\mathbf{A}^{[j]} &= \sigma(\mathbf{Z}^{[j]})
\end{align}$$
"""

# â•”â•â•¡ 410ff860-ffc4-11ea-1d71-6141309e0112
function update!(nn, Î±=1.2)
	for i in 1:length(nn.activation_functions)	
		Wi = nn.Î¸["W$i"]
		bi = nn.Î¸["b$i"]

		dWi = nn.grads["dW$i"]
		dbi = nn.grads["db$i"]

		nn.Î¸["W$i"] = Wi .- Î±*dWi
		nn.Î¸["b$i"] = bi .- Î±*dbi
	end
end

# â•”â•â•¡ bd826122-ffc5-11ea-05de-9995d65c1b86
function train!(nn, X, Y, niters=10000)	
	initialize!(nn, X, Y)
	
	for i in 1:niters
		forwardpass!(nn, X)
		â„’ = cost(nn, Y)
		backpropagation!(nn, X, Y)
		update!(nn)
	end

	return nn
end

# â•”â•â•¡ 0dee25e0-ffc6-11ea-0b20-3f4781d6ff83
function predict(nn, X)
	forwardpass!(nn, X)
	N = length(nn.activation_functions)
	Aâ‚™ = nn.forward["A$N"]
	predictions = Aâ‚™ .> 0.5
	return predictions
end

# â•”â•â•¡ 2504f1ee-ffc6-11ea-3b9a-d17dcf5d9b6e
mean(predict(nn, X))

# â•”â•â•¡ 7af70d00-ffc6-11ea-1bd9-ddb90f5b04cd
begin
	trigger = nothing
	train!(nn, X, Y)
end

# â•”â•â•¡ 1870d340-ffcc-11ea-12c1-c7765b8a2957
md"### Accuracy"

# â•”â•â•¡ 03a3e210-ffca-11ea-26e3-132e96817f3c
begin
	trigger
	predictions = predict(nn, X)
	sum(predictions .== Y) / length(Y) * 100
end

# â•”â•â•¡ 1f400f30-ffca-11ea-3ef6-1b1722d88043
# (Y * predictions' + (1 .- Y) * (1 .- predictions')) / length(Y)

# â•”â•â•¡ 859b8c80-ffc7-11ea-1590-2503626327ab
function meshgrid(x, y)
    X = [i for i in x, j in 1:length(y)]
    Y = [j for i in 1:length(x), j in y]
    return X, Y
end

# â•”â•â•¡ 89572a60-ffc6-11ea-2480-af31c91aac6c
function plot_decision_boundary(model, X, y)
    # Set min and max values and give it some padding
    x_min, x_max = minimum(X[1,:]) - 1, maximum(X[1,:]) + 1
    y_min, y_max = minimum(X[2,:]) - 1, maximum(X[2,:]) + 1
    h = 0.01

	# Generate a grid of points with distance h between them
    xx, yy = meshgrid(x_min:h:x_max, y_min:h:x_max)

	# Predict the function value for the whole grid
    Z = map((x,y)->model([x, y])[1], xx, yy)

	# Plot the contour and training examples
	plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel("x2")
    plt.xlabel("x1")
    plt.scatter(X[1,:], X[2,:], c=y, cmap=plt.cm.Spectral)
end

# â•”â•â•¡ 96a71ad0-ffc7-11ea-069d-1f4e1916d2ab
plot_decision_boundary(x->predict(nn, x), X, Y); gcf()

# â•”â•â•¡ 41cda7ee-fea4-11ea-32d8-677bd3f7bc0b
md"## Examples"

# â•”â•â•¡ 4533b650-fea4-11ea-329b-9363cdc0a214
begin
    function test_neural_network(g=Ïƒ)
        x = 2
        Ï† = x -> [x, x^2, sqrt(abs(x))]
        ğ• = [[2,-1,3], [3,0,1]]
        ğ° = [+1, -1]
        neural_network(x, ğ•, ğ°, Ï†, g)
    end

    @info test_neural_network(Ïƒ) â‰ˆ -0.013563772681566943
    @info test_neural_network(ReLU) â‰ˆ -3.1715728752538093

    @info Ïƒ(0) == 0.5
    @info ReLU(1) == 1
    @info ReLU(-1) == 0
end

# â•”â•â•¡ 5d285f40-fea4-11ea-3295-b7fa7064eaae
begin
    function test_two_layer_neural_network(ğ =[Ïƒ])
        x = 2
        Ï† = x -> [x, x^2, sqrt(abs(x))]
        ğ• = [[2,-1,3], [3,0,1]]
        ğ° = [+1, -1]
        ğ– = [ğ•, ğ°]
        multi_layer_neural_network(x, ğ–, Ï†, ğ )
    end

    function test_five_layer_neural_network(ğ )
        x = 2
        Ï† = x -> [x, x^2, sqrt(abs(x))]
        ğ•â‚ = [[2,-1,3], [3,0,1], [7,5,3]]
        ğ•â‚‚ = [[6,5,9], [3,3,3]]
        ğ•â‚ƒ = [[6,5], [3,3], [3,3], [3,3], [3,3]]
        ğ•â‚„ = [[1,2,3,4,5], [6,7,8,9,0]]
        ğ° = [+1, -1]
        ğ– = [ğ•â‚, ğ•â‚‚, ğ•â‚ƒ, ğ•â‚„, ğ°]
        multi_layer_neural_network(x, ğ–, Ï†, ğ )
    end

    @info test_two_layer_neural_network([Ïƒ]) â‰ˆ -0.013563772681566943
    @info test_two_layer_neural_network([ReLU]) â‰ˆ -3.1715728752538093
    @info test_five_layer_neural_network([Ïƒ,Ïƒ,Ïƒ,Ïƒ]) â‰ˆ -3.1668639943749355e-7
end

# â•”â•â•¡ Cell order:
# â•Ÿâ”€dbd0ccc0-fea3-11ea-359e-876ee15179be
# â• â•0b7610be-fea4-11ea-0a69-eff0a019b6e6
# â•Ÿâ”€047d3eb2-fea4-11ea-3ff7-0b15c54073e2
# â• â•08c79290-fea4-11ea-0a77-e5f5e6ab401d
# â• â•c8f6b2c0-ffd1-11ea-3972-25233fa64f94
# â• â•1384e6b0-fea4-11ea-2e03-cdad79f986d5
# â• â•ae23a200-ffd1-11ea-3cfe-bb895cf90087
# â• â•be738170-ffd1-11ea-23ec-45adb917e9c6
# â•Ÿâ”€233ec7b0-fea4-11ea-3a47-a3125ec028be
# â•Ÿâ”€a13c0ba0-fea4-11ea-0fa7-cde0a3840c99
# â• â•11ab44b0-fea4-11ea-042f-a3547f3e2a8b
# â•Ÿâ”€12cfa070-fea4-11ea-0963-9548b2c07c44
# â• â•3c132240-fea4-11ea-0b96-8fac4ceb5841
# â• â•32b06dc0-fef4-11ea-25f2-cdaeafd52ab4
# â•Ÿâ”€a43062c0-0020-11eb-245e-cf6b2036b878
# â• â•b17c5100-0020-11eb-2928-81d80731960e
# â•Ÿâ”€c43d5c20-ff9f-11ea-15ce-3bef26583d2a
# â•Ÿâ”€492db7ae-ff9e-11ea-38a0-09cead1c5ae6
# â•Ÿâ”€e7ffb7c0-ff9f-11ea-3c0e-cbb878af2c6e
# â•Ÿâ”€180dc51e-ff9f-11ea-3ad0-a51012f662b0
# â•Ÿâ”€07054ea0-ffa0-11ea-1fd1-6f70c3600784
# â•Ÿâ”€d75e9920-ffa1-11ea-1bb0-6d0424b2b5b5
# â•Ÿâ”€b70c8f9e-ffa2-11ea-1f51-cd5660e3eb37
# â•Ÿâ”€0e09c0d0-ff9d-11ea-0e5e-37f227129043
# â•Ÿâ”€e3dd9740-ff9d-11ea-0d04-ad6f8be98423
# â•Ÿâ”€385750e0-ffa3-11ea-03d1-a7cc682d4ab2
# â•Ÿâ”€e2891c10-ffa3-11ea-05a6-f93d0bd8daf8
# â•Ÿâ”€66219680-ffa7-11ea-3528-9184d31f42de
# â•Ÿâ”€30cb2962-ffac-11ea-114f-73601457cef0
# â•Ÿâ”€c708c540-ffac-11ea-0632-35fdb0b7a4ef
# â•Ÿâ”€596a9110-ffae-11ea-2c89-7518df5b04c6
# â• â•3945e080-ffac-11ea-23a8-b968fe84d903
# â• â•35d0b6f0-ffac-11ea-27b1-47b519c13c21
# â•Ÿâ”€a2bcbdb0-ffa0-11ea-0a41-457fa1b70946
# â•Ÿâ”€68ca10f0-ff49-11ea-3147-09eb71687f20
# â•Ÿâ”€24692920-ffb0-11ea-3ab2-47b00719deed
# â•Ÿâ”€5a430f62-ffb1-11ea-2123-cd71772704ee
# â• â•3641cda0-ffbf-11ea-27a3-5fe417918cc0
# â• â•991f6770-ffbf-11ea-048c-1113d8640637
# â• â•7438c7c0-001a-11eb-1b09-b593715f3e82
# â• â•55eb5030-ffc0-11ea-0688-c1ecd12f1939
# â• â•98799310-ffd6-11ea-0714-17b135c4c704
# â• â•efce9b80-ffc0-11ea-2cb3-4f8209f29cdc
# â• â•10fb310e-ffc1-11ea-0d21-539799c2aebf
# â• â•3d66f4f0-ffc1-11ea-18ae-a5063f76ded5
# â• â•4016f9c0-ffc1-11ea-0638-8fc7e9f2c52b
# â• â•3c930220-ffcc-11ea-0694-e73b06bf7623
# â• â•6c1a4160-ffcd-11ea-249a-79972ec4eb85
# â• â•d8083910-ffc1-11ea-31f8-07d7fd4a0b9a
# â• â•1aaf8430-ffc2-11ea-0520-451f13594ddc
# â• â•b6d126c0-ffc2-11ea-2400-9bd611d2d8b7
# â• â•92237430-ffd7-11ea-3356-d9cd94e2a587
# â• â•a19c1100-ffd8-11ea-37d1-b3f2e0b6dc0f
# â• â•ea7a6f60-ffd9-11ea-395c-f769cabcc23f
# â• â•ba7f4af0-0020-11eb-2112-e13c9ebaec6c
# â• â•835e1c70-ffc3-11ea-31a5-214731bbc366
# â• â•0414b960-ffd7-11ea-313c-3fbce53d7a70
# â•Ÿâ”€24fd6bb0-0025-11eb-0489-5f1193474b18
# â• â•410ff860-ffc4-11ea-1d71-6141309e0112
# â• â•bd826122-ffc5-11ea-05de-9995d65c1b86
# â• â•0dee25e0-ffc6-11ea-0b20-3f4781d6ff83
# â• â•2504f1ee-ffc6-11ea-3b9a-d17dcf5d9b6e
# â• â•7af70d00-ffc6-11ea-1bd9-ddb90f5b04cd
# â•Ÿâ”€1870d340-ffcc-11ea-12c1-c7765b8a2957
# â• â•03a3e210-ffca-11ea-26e3-132e96817f3c
# â• â•1f400f30-ffca-11ea-3ef6-1b1722d88043
# â•Ÿâ”€859b8c80-ffc7-11ea-1590-2503626327ab
# â•Ÿâ”€89572a60-ffc6-11ea-2480-af31c91aac6c
# â• â•96a71ad0-ffc7-11ea-069d-1f4e1916d2ab
# â•Ÿâ”€41cda7ee-fea4-11ea-32d8-677bd3f7bc0b
# â• â•4533b650-fea4-11ea-329b-9363cdc0a214
# â• â•5d285f40-fea4-11ea-3295-b7fa7064eaae
