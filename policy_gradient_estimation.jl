### A Pluto.jl notebook ###
# v0.12.3

using Markdown
using InteractiveUtils

# â•”â•â•¡ b039225e-0f65-11eb-0925-337baf99a1f2
using Pkg; Pkg.add("AddPackage"); using AddPackage

# â•”â•â•¡ 7e7a7440-0f65-11eb-14e5-e1281ae10f18
@add using Distributions

# â•”â•â•¡ 96f42b80-0f40-11eb-1fde-67a6c8b9aad4
md"""
# Policy Gradient Estimation
To make improvements to a policy $\pi$, one way to inform *how* to improve the policy is to estimate the utility gradient $\nabla U$ with respect to the policy parameters $ğ›‰$ to guide the optimization process.
"""

# â•”â•â•¡ a2114652-0f55-11eb-3324-81f9b428185a
md"""
## Markov Decision Process (MDP)
The 5-tuple that defines a *Markov decision process* is the following.

$$\langle \mathcal S, \mathcal A, T, R, \gamma \rangle$$

Variable           | Description
:----------------- | :--------------
$\mathcal{S}$      | State space
$\mathcal{A}$      | Action space
$T$                | Transition model
$R$                | Reward model
$\gamma \in [0,1]$ | Discount factor

We also define $TR$ as a shorthand to sample the transition model $T$ and reward model $R$.
"""

# â•”â•â•¡ 8c5079e0-0f40-11eb-0c79-4ddc395519a3
struct MDP
	ğ’®  # state space
	ğ’œ  # action space
	T  # transition function
	R  # reward function
	TR # sample transition and reward
	Î³  # discount factor
end

# â•”â•â•¡ 59285fa0-0f5f-11eb-323e-03e29ded12df
md"""
## Working Example: Gridworld MDP
"""

# â•”â•â•¡ 1612cce0-0f60-11eb-1336-2939d0577246
# using TikzPictures

# â•”â•â•¡ d402751e-0f60-11eb-02ea-a5a92a50c5ee
md"""
Simple MDP example from [Josh Greaves](https://joshgreaves.com/reinforcement-learning/introduction-to-reinforcement-learning/).
"""

# â•”â•â•¡ 271e0d10-0f60-11eb-0e1a-a10dd335701f
# TikzPicture(L"""
# \node[minimum size=1cm, draw=black, fill=white, circle] (s) {\shortstack{Don't\\Understand}};
# \node[minimum size=1cm, draw=black, fill=white, circle, right=3.5cm of s] (s2) {Understand};
# \node[minimum size=1.3cm, draw=black, fill=white, diamond, above=0.5cm of s] (r) {$r_t$};
# \node[minimum size=1cm, draw=black, fill=white, rectangle, above=0.5cm of r] (a) {$a_t$};


# \draw[->] (s) -- (s2) node[midway, above] {Study $(0.8)$};
# \draw[->] [loop below, label={Study $(0.2)$}] (s) to (s);
# %\path[->] (s) -- [out=180, in=200, looseness=5, label={Study $(0.2)$}] (s);

# %\path[->]
# %    (s) edge [out=0] node {Study} (s2);
# %\draw[->] (s) -- (s2) node {Study};

# \draw[->] (s) -- (r);
# \draw[->] (a) -- (r);
# \draw[->] (a) [out=0,in=135] to (s2);
# """,
# preamble="""
# \\definecolor{pastelBlue}{HTML}{1BA1EA}
# \\usetikzlibrary{shapes}
# \\usetikzlibrary{positioning}
# \\usetikzlibrary{arrows}
# \\tikzset{every picture/.style={semithick, >=stealth'}}
# """,
# options="every node/.style={scale=1.25}")

# â•”â•â•¡ 89d06640-0f62-11eb-1e65-192bc5b08a51
md"""
### States
$$\mathcal{S} = \{\text{don't understand},\; \text{understand}\}$$
"""

# â•”â•â•¡ 5cc50e60-0f5f-11eb-0e06-2b69b7017802
@enum States dont_understand understand

# â•”â•â•¡ 12013c40-0f60-11eb-31a6-0733ce4a66fb
ğ’® = (dont_understand, understand);

# â•”â•â•¡ ff3b56f0-0f63-11eb-0689-1985d62a2f7a
md"""
### Actions
$$\mathcal{A} = \{\text{don't study},\; \text{study}\}$$
"""

# â•”â•â•¡ 01287dbe-0f60-11eb-298a-93199a217f0f
@enum Actions dont_study study

# â•”â•â•¡ 0873b592-0f60-11eb-1961-b955934386b5
ğ’œ = (dont_study, study);

# â•”â•â•¡ 1a07d800-0f64-11eb-3d54-3994bcf81271
md"""
### Rewards
We get a $-1$ reward for studying (it's tough...) and a $+1$ reward for not studying. We also get an additive $+10$ reward for understanding and a $-10$ reward for not understanding.
"""

# â•”â•â•¡ 20f53860-0f64-11eb-15e6-cb9384937408
R(s,a) = (a == study ? -1 : 1) + (s == understand ? 10 : -10)

# â•”â•â•¡ 430b91b0-0f64-11eb-1d73-89c89c9bb7bb
md"""
### Transitions
We understand a concept $80\%$ of the time when we study.
"""

# â•”â•â•¡ 8cb37710-0f64-11eb-0d21-abe7e504ff22
function T(s,a)
	if s == dont_understand && a == dont_study
		# if we don't understand and don't study, we continue to not understand.
		return Categorical([1.0, 0.0])
	elseif s == dont_understand && a == study
		# if we don't understand and study, we have an 80% chance to understand.
		return Categorical([0.2, 0.8])
	else
		# otherwise, we understand already.
		return Categorical([0.0, 1.0])
	end
end

# â•”â•â•¡ 2baa0b90-0f65-11eb-0526-0facc81c6edd
function TR(s,a)
	sâ€² = ğ’®[rand(T(s,a))]
	r = R(sâ€²,a)
	return (sâ€², r)
end	

# â•”â•â•¡ 1872c3a0-0f65-11eb-02d9-757cd36e2f65
Î³ = 0.95 # discount factor

# â•”â•â•¡ 216ac110-0f65-11eb-0991-f9fc5d96c2cb
ğ’« = MDP(ğ’®, ğ’œ, T, R, TR, Î³)

# â•”â•â•¡ d8814fe0-0f47-11eb-1b8b-699d58ab9347
md"""
## Finite Difference

$$\frac{\partial f}{\partial x}(x) \approx \frac{f(x + \delta) - f(x)}{Î´}$$
"""

# â•”â•â•¡ c8d79b80-0f47-11eb-0c2b-793cbd7ea9dd
forward_diff(f, x; Î´=sqrt(eps())) = (f(x + Î´) - f(x)) / Î´

# â•”â•â•¡ 2cf4ba30-0f48-11eb-260d-413f902c1460
md"""
Example approximate derivative:

$\frac{\partial \log(x^2)}{\partial x} = \frac{2x}{x^2}$
"""

# â•”â•â•¡ bbe3c2e2-0f48-11eb-37d1-21a6be0e06ed
f(x) = log(x^2);

# â•”â•â•¡ ec43cb60-0f48-11eb-1ca9-918754d22908
fâ€²(x) = 2x/x^2;

# â•”â•â•¡ 4fc84900-0f48-11eb-323a-0755b22da27f
x = 3;

# â•”â•â•¡ 03b94eb0-0f48-11eb-125e-c51ab377cd05
dx = forward_diff(f, x)

# â•”â•â•¡ 86342b80-0f48-11eb-1588-87b1ddba4376
isapprox(dx, fâ€²(x), atol=1e-5)

# â•”â•â•¡ 10d006b2-0f49-11eb-2553-97020ab3d986
md"""
### Value Gradient Estimate

In the context of policy optimization, we want to estimate the gradient

$$\nabla U(ğ›‰) \approx \begin{bmatrix}
\displaystyle\frac{U(ğ›‰ + \delta \mathbf e^{(1)}) - U(ğ›‰)}{Î´}, \,\ldots\, , \frac{U(ğ›‰ + \delta\mathbf e^{(n)}) - U(ğ›‰)}{\delta}
\end{bmatrix}$$

where $\mathbf e^{(i)}$ is the $i$th *standard basis* vector consisting of zeros except for the $i$th component that is $1$.
"""

# â•”â•â•¡ 9f649350-0f49-11eb-1efc-c34b0435fa68
ğ(i,n) = Int[j==i for j in 1:n];

# â•”â•â•¡ 20ea9df0-0f4c-11eb-2a5b-7b44804d5834
md"""
Example vector-valued function $U(ğ›‰)$, taking the partial derivatives numerically

$$U(ğ›‰) = \theta_1^2 + \theta_2^2$$

where $U: ğ›‰ \to \mathbb{R}$, a real-valued function of $n$ variables.
"""

# â•”â•â•¡ cd98d040-0f4b-11eb-2122-c72e2aaa7c5a
U(ğ›‰) = ğ›‰[1]^2 + ğ›‰[2]^2;

# â•”â•â•¡ d9defc60-0f4d-11eb-3d65-3d1a30081be6
U([1, 2])

# â•”â•â•¡ 6b527e50-0f54-11eb-3485-0701ca2589af
ğ›‰ = [1, 2]

# â•”â•â•¡ 529b85e0-0f4b-11eb-101d-290839143cc5
âˆ‡U(U,ğ›‰; n=length(ğ›‰),Î´=sqrt(eps())) = [forward_diff(U, ğ›‰; Î´=Î´*ğ(i,n))[i] for i in 1:n];

# â•”â•â•¡ acd3d94e-0f4f-11eb-11af-13f83340567d
Markdown.parse("""
\$\$\\begin{equation}
\\nabla U(ğ›‰) = \\begin{bmatrix}
2\\theta_1\\\\
2\\theta_2
	\\end{bmatrix} \\approx \\begin{bmatrix}
$(âˆ‡U(U, ğ›‰)[1])\\\\
$(âˆ‡U(U, ğ›‰)[2])
\\end{bmatrix}
\\end{equation}\$\$
""")

# â•”â•â•¡ df2e9d30-0f4b-11eb-1171-f5e3a7d1eef3
âˆ‡U(U, ğ›‰)

# â•”â•â•¡ 560cc260-0f4c-11eb-2ee6-a58f60d96694
md"""
## Simulate Rollouts
We simulate policy $\pi$ rollouts to estimate the value function $U(ğ›‰)$.
"""

# â•”â•â•¡ 59e14a00-0f4c-11eb-2d2e-9d4263911877
function simulate(ğ’«::MDP, s, Ï€, d)
	Ï„ = []
	for i in 1:d
		a = Ï€(s)
		sâ€², r = ğ’«.TR(s,a)
		push!(Ï„, (s,a,r))
		s = sâ€²
	end
	return Ï„
end

# â•”â•â•¡ 8f5016c0-0f6b-11eb-3f16-832e1dc52a59
md"""
---
"""

# â•”â•â•¡ 9ccbd980-0f69-11eb-3f64-c3f04deff03d
md"""
Example random policy uniformly over actions *don't study* or *study*.
"""

# â•”â•â•¡ b8bb3910-0f50-11eb-3a1f-95fe87fb974f
Ï€(s) = rand(ğ’œ); # random policy

# â•”â•â•¡ eb6b6690-0f65-11eb-028f-73699f3e4372
Ï„ = simulate(ğ’«, dont_understand, Ï€, 10)

# â•”â•â•¡ 928d4060-0f6b-11eb-35ba-e16d91e52bf8
md"""
---
"""

# â•”â•â•¡ b3259720-0f69-11eb-05f2-010da263b15c
md"""
Example policy where we *study* with a $10\%$ chance.
"""

# â•”â•â•¡ 57955800-0f69-11eb-253f-9b9a9956038b
Ï€_90_10(s) = ğ’œ[rand(Categorical([0.9, 0.1]))];

# â•”â•â•¡ 95264cb0-0f69-11eb-02a9-4f1dd18a47fa
Ï„_90_10 = simulate(ğ’«, dont_understand, Ï€_90_10, 10)

# â•”â•â•¡ 94af0ef0-0f6b-11eb-369b-a14fc2d1bc24
md"""
---
"""

# â•”â•â•¡ c0788a40-0f69-11eb-1d93-4f85b2e7116d
md"""
Example policy where we always study if we don't understand.
"""

# â•”â•â•¡ 1abf2f30-0f66-11eb-1b73-93bfdbdd540f
Ï€_study(s) = s == dont_understand ? study : dont_study;

# â•”â•â•¡ 4ec50e32-0f66-11eb-09d7-8bcfbc32b0b5
Ï„_study = simulate(ğ’«, dont_understand, Ï€_study, 10)

# â•”â•â•¡ 4243cf82-0f51-11eb-3c56-4fc7dd540660
md"""
## Regression Gradient
Instead of using the finite difference to approximate the gradient at $ğ›‰$, we can use *linear regression* with random perturbations from $ğ›‰$.

The perturbations are stored in a matrix:

$$\Delta \mathbf{\Theta} = \left[\Delta ğ›‰^{(1)},\, \ldots,\, \Delta ğ›‰^{(m)}\right]$$

For each perturbation, we perform a rollout and estimate the change in utility:

$$\Delta \mathbf U = \left[ U\left(ğ›‰ + \Delta ğ›‰^{(1)}\right) - U(ğ›‰),\, \ldots, \, U\left(ğ›‰ + \Delta ğ›‰^{(m)}\right) - U(ğ›‰)\right]$$

Then the policy gradient estimate using linear regression is:

$$\nabla U(ğ›‰) \approx \Delta \mathbf\Theta^+ \Delta\mathbf U$$
"""

# â•”â•â•¡ 6ed6adf0-0f52-11eb-3680-05adcb13cbda
struct RegressionGradient
	ğ’« # problem (MDP)
	b # initial state distribution
	d # depth
	m # number of samples
	Î´ # step size
end

# â•”â•â•¡ 79439820-0f52-11eb-0efd-335db997a7ad
import LinearAlgebra: normalize

# â•”â•â•¡ 63412240-0f52-11eb-0081-b7703f2feee7
function gradient(M::RegressionGradient, Ï€, ğ›‰)
	ğ’«, b, d, m, Î´, Î³ = M.ğ’« , M.b, M.d, M.m, M.Î´, M.ğ’«.Î³
	Î”ğš¯ = [Î´.*normalize(randn(length(ğ›‰)), 2) for i = 1:m]
	R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
	U(ğ›‰) = R(simulate(ğ’«, rand(b), s->Ï€(ğ›‰,s), d))
	Î”U = [U(ğ›‰ + Î”ğ›‰) - U(ğ›‰) for Î”ğ›‰ in Î”ğš¯]
	return pinv(reduce(hcat, Î”ğš¯)') * Î”U
end

# â•”â•â•¡ 78347dd0-0f6d-11eb-06dd-cf7168b0b81d
M = RegressionGradient(ğ’«, ğ’«.ğ’®, 10, 20, 0.1)

# â•”â•â•¡ cf75a260-0f52-11eb-3bc9-3d30c93d8fba
md"""
## Likelihood Ratio

The *likelihood ratio trick* is:

$$\nabla_ğ›‰ \log p_ğ›‰(\tau) = \frac{\nabla_ğ›‰ p_ğ›‰(\tau)}{p_ğ›‰(\tau)}$$

Which can be used to evaluate the policy gradient:

$$\begin{align}
\nabla U(ğ›‰) &= \mathbb{E}_\tau \left[\nabla_ğ›‰ \log p_ğ›‰(\tau) R(\tau)\right] 
\end{align}$$

We can estimate the above expectation through rolling out simulated trajectories.
"""

# â•”â•â•¡ 1bdc2700-0f5d-11eb-1152-4bb2534f0f46
struct LikelihoodRatioGradient
	ğ’« # problem (MDP)
	b # initial state distribution
	d # depth
	m # number of samples
	âˆ‡logÏ€ # gradient of log likelihood
end

# â•”â•â•¡ 53f1e8f0-0f5d-11eb-0f27-677b660be61e
import Statistics: mean

# â•”â•â•¡ 27223320-0f5d-11eb-200e-9fa83fc1db57
function gradient(M::LikelihoodRatioGradient, Ï€, ğ›‰)
	ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
	Ï€ğ›‰(s) = Ï€(ğ›‰, s)
	R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
	âˆ‡U(Ï„) = sum(âˆ‡logÏ€(ğ›‰, a, s) for (s,a) in Ï„)*R(Ï„)
	return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€ğ›‰, d)) for i in 1:m)
end

# â•”â•â•¡ 077527c0-0f54-11eb-0bb9-417408e519b5
md"""
## Reward-to-Go

The *reward-to-go* approach attempts to reduce the variance in the estimate, as compared to the likeihood ration policy gradient method.

$$\nabla U(ğ›‰) = \mathbb{E}_\tau \left[ \sum_{k=1}^d \nabla_ğ›‰\log \pi_ğ›‰\left(s^{(k)} \mid s^{(k)}\right) \gamma^{k-1} r_\text{to-go}^{(k)} \right]$$

with the reward-to-go from step $k$ defined as:

$$r_\text{to-go}^{(k)} = \sum_{\ell=k}^dr^{(\ell)}\gamma^{\ell-k}$$
"""

# â•”â•â•¡ 696a6e50-0f5d-11eb-0c53-87841669a060
struct RewardToGoGradient
	ğ’« # problem
	b # initial state distribution
	d # depth
	m # number of samples
	âˆ‡logÏ€ # gradient of log likelihood
end

# â•”â•â•¡ 6ce76010-0f5d-11eb-39d8-37881e2d6650
function gradient(M::RewardToGoGradient, Ï€, ğ›‰)
	ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
	Ï€ğ›‰(s) = Ï€(ğ›‰, s)
	R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
	âˆ‡U(Ï„) = sum(âˆ‡logÏ€(ğ›‰, a, s)*R(Ï„,j) for (j, (s,a,r)) in enumerate(Ï„))
	return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€ğ›‰, d)) for i in 1:m)
end

# â•”â•â•¡ 91d2e650-0f45-11eb-2baf-998fe3df3270
md"""
## Advantage

$$A(s,a) = Q(s,a) - U(s)$$

The policy gradient using the advantage is unbiases and typically has much lower variance. The gradient compuation has the form:

$$\nabla U(ğ›‰) = \mathbb{E}_\tau\left[ \sum_{k=1}^d \nabla_ğ›‰ \log \pi_ğ›‰\left(a^{(k)} \mid s^{(k)}\right) \gamma^{k-1} A_ğ›‰\left(s^{(k)}, a^{(k)}\right) \right]$$
"""

# â•”â•â•¡ Cell order:
# â•Ÿâ”€96f42b80-0f40-11eb-1fde-67a6c8b9aad4
# â• â•b039225e-0f65-11eb-0925-337baf99a1f2
# â•Ÿâ”€a2114652-0f55-11eb-3324-81f9b428185a
# â• â•8c5079e0-0f40-11eb-0c79-4ddc395519a3
# â•Ÿâ”€59285fa0-0f5f-11eb-323e-03e29ded12df
# â• â•1612cce0-0f60-11eb-1336-2939d0577246
# â•Ÿâ”€d402751e-0f60-11eb-02ea-a5a92a50c5ee
# â•Ÿâ”€271e0d10-0f60-11eb-0e1a-a10dd335701f
# â•Ÿâ”€89d06640-0f62-11eb-1e65-192bc5b08a51
# â• â•5cc50e60-0f5f-11eb-0e06-2b69b7017802
# â• â•12013c40-0f60-11eb-31a6-0733ce4a66fb
# â•Ÿâ”€ff3b56f0-0f63-11eb-0689-1985d62a2f7a
# â• â•01287dbe-0f60-11eb-298a-93199a217f0f
# â• â•0873b592-0f60-11eb-1961-b955934386b5
# â•Ÿâ”€1a07d800-0f64-11eb-3d54-3994bcf81271
# â• â•20f53860-0f64-11eb-15e6-cb9384937408
# â•Ÿâ”€430b91b0-0f64-11eb-1d73-89c89c9bb7bb
# â• â•7e7a7440-0f65-11eb-14e5-e1281ae10f18
# â• â•8cb37710-0f64-11eb-0d21-abe7e504ff22
# â• â•2baa0b90-0f65-11eb-0526-0facc81c6edd
# â• â•1872c3a0-0f65-11eb-02d9-757cd36e2f65
# â• â•216ac110-0f65-11eb-0991-f9fc5d96c2cb
# â•Ÿâ”€d8814fe0-0f47-11eb-1b8b-699d58ab9347
# â• â•c8d79b80-0f47-11eb-0c2b-793cbd7ea9dd
# â•Ÿâ”€2cf4ba30-0f48-11eb-260d-413f902c1460
# â• â•bbe3c2e2-0f48-11eb-37d1-21a6be0e06ed
# â• â•ec43cb60-0f48-11eb-1ca9-918754d22908
# â• â•4fc84900-0f48-11eb-323a-0755b22da27f
# â• â•03b94eb0-0f48-11eb-125e-c51ab377cd05
# â• â•86342b80-0f48-11eb-1588-87b1ddba4376
# â•Ÿâ”€10d006b2-0f49-11eb-2553-97020ab3d986
# â• â•9f649350-0f49-11eb-1efc-c34b0435fa68
# â• â•529b85e0-0f4b-11eb-101d-290839143cc5
# â•Ÿâ”€20ea9df0-0f4c-11eb-2a5b-7b44804d5834
# â• â•cd98d040-0f4b-11eb-2122-c72e2aaa7c5a
# â• â•d9defc60-0f4d-11eb-3d65-3d1a30081be6
# â• â•6b527e50-0f54-11eb-3485-0701ca2589af
# â•Ÿâ”€acd3d94e-0f4f-11eb-11af-13f83340567d
# â• â•df2e9d30-0f4b-11eb-1171-f5e3a7d1eef3
# â•Ÿâ”€560cc260-0f4c-11eb-2ee6-a58f60d96694
# â• â•59e14a00-0f4c-11eb-2d2e-9d4263911877
# â•Ÿâ”€8f5016c0-0f6b-11eb-3f16-832e1dc52a59
# â•Ÿâ”€9ccbd980-0f69-11eb-3f64-c3f04deff03d
# â• â•b8bb3910-0f50-11eb-3a1f-95fe87fb974f
# â• â•eb6b6690-0f65-11eb-028f-73699f3e4372
# â•Ÿâ”€928d4060-0f6b-11eb-35ba-e16d91e52bf8
# â•Ÿâ”€b3259720-0f69-11eb-05f2-010da263b15c
# â• â•57955800-0f69-11eb-253f-9b9a9956038b
# â• â•95264cb0-0f69-11eb-02a9-4f1dd18a47fa
# â•Ÿâ”€94af0ef0-0f6b-11eb-369b-a14fc2d1bc24
# â•Ÿâ”€c0788a40-0f69-11eb-1d93-4f85b2e7116d
# â• â•1abf2f30-0f66-11eb-1b73-93bfdbdd540f
# â• â•4ec50e32-0f66-11eb-09d7-8bcfbc32b0b5
# â•Ÿâ”€4243cf82-0f51-11eb-3c56-4fc7dd540660
# â• â•6ed6adf0-0f52-11eb-3680-05adcb13cbda
# â• â•79439820-0f52-11eb-0efd-335db997a7ad
# â• â•63412240-0f52-11eb-0081-b7703f2feee7
# â• â•78347dd0-0f6d-11eb-06dd-cf7168b0b81d
# â•Ÿâ”€cf75a260-0f52-11eb-3bc9-3d30c93d8fba
# â• â•1bdc2700-0f5d-11eb-1152-4bb2534f0f46
# â• â•53f1e8f0-0f5d-11eb-0f27-677b660be61e
# â• â•27223320-0f5d-11eb-200e-9fa83fc1db57
# â•Ÿâ”€077527c0-0f54-11eb-0bb9-417408e519b5
# â• â•696a6e50-0f5d-11eb-0c53-87841669a060
# â• â•6ce76010-0f5d-11eb-39d8-37881e2d6650
# â•Ÿâ”€91d2e650-0f45-11eb-2baf-998fe3df3270
